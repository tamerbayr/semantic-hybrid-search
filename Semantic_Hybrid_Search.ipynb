{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# --Eƒüitim b√∂l√ºm√º. Direkt kullanƒ±m b√∂l√ºm√ºne ge√ßmek i√ßin a≈üaƒüƒ± inin."
      ],
      "metadata": {
        "id": "zTva3pQx8fBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q sentence-transformers datasets pandas scikit-learn faiss-cpu gradio stopwords_tr"
      ],
      "metadata": {
        "id": "RQROz2wmnJwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7zFoqSyVhwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2ebe53-1ae5-4107-fcdc-25b09f77be83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import html\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import faiss\n",
        "import pickle\n",
        "import numpy as np\n",
        "from os import makedirs, path, environ\n",
        "\n",
        "environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache/datasets\"\n",
        "environ[\"HF_HOME\"] = \"./hf_cache\"\n",
        "\n",
        "print(\"ok\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eƒüitim (huggingface token ve verisetine eri≈üim gerekiyor).\n",
        "# verisetini indirmek i√ßin hf'de onay vermeniz gerekiyor. https://huggingface.co/datasets/alibayram/hukuk_soru_cevap/tree/main\n",
        "login()\n",
        "\n",
        "makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"alibayram/hukuk_soru_cevap\",\n",
        "    split=\"train\",\n",
        ")\n",
        "\n",
        "embed_model = SentenceTransformer(\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "#embed\n",
        "sorular = [str(s) for s in dataset[\"soru\"]]\n",
        "cevaplar = [str(c) for c in dataset[\"cevap\"]]\n",
        "\n",
        "embeddings = embed_model.encode(\n",
        "    sorular,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "index.add(embeddings)\n",
        "\n",
        "#kayƒ±t\n",
        "makedirs(\"artifacts\", exist_ok=True)\n",
        "faiss.write_index(index, \"artifacts/faiss.index\")\n",
        "with open(\"artifacts/sorular.pkl\", \"wb\") as f:\n",
        "    pickle.dump(sorular, f)\n",
        "\n",
        "with open(\"artifacts/cevaplar.pkl\", \"wb\") as f:\n",
        "    pickle.dump(cevaplar, f)"
      ],
      "metadata": {
        "id": "qt95Fqltb65s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#veriseti y√ºklenme kontrol√º\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "df = df[[\"soru\", \"cevap\"]].dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(\"y√ºklenen √∂rnek sayƒ±sƒ±:\", len(df))\n",
        "df.head()\n",
        "\n",
        "print(\"ok\")"
      ],
      "metadata": {
        "id": "6wycOr1k9lfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "07ebca94-bb6f-419e-9895-bfa6aeb019c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2639625722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#veriseti y√ºklenme kontrol√º\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"soru\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cevap\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = df[\"soru\"].tolist()\n",
        "dim = embeddings.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "index.add(embeddings)\n"
      ],
      "metadata": {
        "id": "SFuwhT_bnVHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --test bloƒüu (bunun √ºst√ºndekileri √ßalƒ±≈ütƒ±rmanƒ±za gerek yok. Eƒüitim yapmadƒ±ysanƒ±z √∂nce artifact dosyasƒ±nƒ±n i√ßeriƒüini sol taraftaki klas√∂r simgesine y√ºkleyin.)"
      ],
      "metadata": {
        "id": "OkuwCgq3dhx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers faiss-cpu gradio stopwords_tr"
      ],
      "metadata": {
        "id": "dCR7MMCXdfsU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stopwords_tr as stp\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import pickle\n",
        "from os import path\n",
        "import gradio as gr\n",
        "import html\n",
        "\n",
        "embed_model = SentenceTransformer(\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "\n",
        "if path.exists(\"/content/artifacts/faiss.index\"):\n",
        "    embed_yolu = \"/content/artifacts\"\n",
        "elif path.exists(\"/content/faiss.index\"):\n",
        "    embed_yolu = \"/content\"\n",
        "else:\n",
        "    raise FileNotFoundError(\"Dosyalar bulunamadƒ±. faiss.index, sorular.pkl ve cevaplar.pkl dosyalarƒ±nƒ± y√ºklediƒüinizden emin olun. y√ºkl√º ise oturumu tekrar ba≈ülatmayƒ± deneyin\")\n",
        "\n",
        "index = faiss.read_index(path.join(embed_yolu, \"faiss.index\"))\n",
        "\n",
        "with open(path.join(embed_yolu, \"sorular.pkl\"), \"rb\") as f:\n",
        "    sorular = pickle.load(f)\n",
        "\n",
        "with open(path.join(embed_yolu, \"cevaplar.pkl\"), \"rb\") as f:\n",
        "    cevaplar = pickle.load(f)\n",
        "\n",
        "print(f\"{len(sorular)} adet soru y√ºklendi.\")\n",
        "\n",
        "#cevap alma fonksiyonu\n",
        "def metin_temizle(text):\n",
        "    #basit √∂ni≈üleme\n",
        "    text = text.replace(\"ƒ∞\", \"i\").replace(\"I\", \"ƒ±\").lower()\n",
        "    for punc in [\".\", \",\", \"?\", \"!\", \":\", \";\", \"(\", \")\", \"\\\"\", \"'\", \"-\"]:\n",
        "        text = text.replace(punc, \" \")\n",
        "\n",
        "    kelimeler = text.split()\n",
        "    temiz_kelimeler = [k for k in kelimeler if not k in stp.stopwords() and len(k) > 2]\n",
        "\n",
        "    return temiz_kelimeler\n",
        "\n",
        "def sonuc_getir(soru, top_k=5, min_score=0.45):\n",
        "    #stopwordslarƒ± filtrele\n",
        "    temiz_kelimeler = metin_temizle(soru)\n",
        "    n_temiz_kelime = len(temiz_kelimeler)\n",
        "\n",
        "    # kelime e≈üle≈üme skoru i√ßin aday havuzu geni≈ü\n",
        "    query_vec = embed_model.encode([soru], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    adaylar = 50\n",
        "    scores, i = index.search(query_vec, adaylar)\n",
        "\n",
        "    sonuc = []\n",
        "\n",
        "    for score, idx in zip(scores[0], i[0]):\n",
        "        idx = int(idx)\n",
        "        db_soru = sorular[idx]\n",
        "        db_cevap = cevaplar[idx]\n",
        "\n",
        "        Bscore = float(score)\n",
        "\n",
        "        # Temizlik\n",
        "        db_soru = db_soru.replace(\"ƒ∞\", \"i\").replace(\"I\", \"ƒ±\").lower()\n",
        "\n",
        "        # Kelime E≈üle≈ümesi Sayma\n",
        "        matchs = 0\n",
        "        for kelime in temiz_kelimeler:\n",
        "            if kelime in db_soru:\n",
        "                matchs += 1\n",
        "\n",
        "        if n_temiz_kelime > 0:\n",
        "            #sorudaki kelimelerin ne kadarƒ± ge√ßiyorsa puan o kadar artƒ±rƒ±lƒ±yor\n",
        "            if matchs > 0:\n",
        "                Bscore = Bscore + (matchs / n_temiz_kelime * 0.30)\n",
        "            #ortak kelime yoksa puan d√º≈ü√ºr\n",
        "            elif Bscore < 0.40:\n",
        "                Bscore = Bscore * 0.8\n",
        "\n",
        "        # eleme\n",
        "        if Bscore < min_score:\n",
        "            continue\n",
        "\n",
        "        sonuc.append({\n",
        "            \"soru\": db_soru,\n",
        "            \"cevap\": db_cevap,\n",
        "            \"score\": min(Bscore, 1.0),\n",
        "            \"matches\": matchs\n",
        "        })\n",
        "\n",
        "    # sƒ±ralama\n",
        "    sonuc = sorted(sonuc, key=lambda x: x[\"score\"], reverse=True)\n",
        "    return sonuc[:top_k]\n",
        "\n",
        "#aray√ºz\n",
        "sorusayisi = 5 #sorulan soruya en fazla ka√ß yanƒ±t g√∂sterilsin\n",
        "\n",
        "def forum_asistani(soru):\n",
        "    if not soru or not soru.strip():\n",
        "        return \"Bir soru girin.\"\n",
        "\n",
        "    try:\n",
        "        sonuc = sonuc_getir(soru, top_k=sorusayisi, min_score=0.45)\n",
        "    except NameError:\n",
        "        return \"Bir hata olu≈ütu. √∂nceki h√ºcreleri veya oturumu tekrar ba≈ülatmayƒ± deneyin.\"\n",
        "\n",
        "    if sonuc is None:\n",
        "        return \"**Bu soru i√ßin yeterli bilgi bulunmamaktadƒ±r.**\\n\\n\"\n",
        "\n",
        "    # sonu√ß g√∂sterme\n",
        "    md = []\n",
        "    md.append(f\"# Sonu√ßlar \\n\")\n",
        "    md.append(f\"**Soru:** {html.escape(soru)}\\n\\n---\\n\")\n",
        "\n",
        "    for i, r in enumerate(sonuc, start=1):\n",
        "        matched_q = html.escape(r.get(\"soru\", \"\"))\n",
        "        answer = r.get(\"cevap\", \"\")\n",
        "        score = min(r.get(\"score\", 0.0), 1.0)\n",
        "\n",
        "        section = (\n",
        "            f\"### Sonu√ß {i}  (Benzerlik: {score:.2f})\\n\\n\"\n",
        "            f\"**Cevap verilen soru:** {matched_q}\\n\\n\"\n",
        "            f\"**Cevap:** {answer}\\n\\n\"\n",
        "            f\"**Kaynak:** [Kaynak soru(demoda hen√ºz eklenmedi)](#)\\n\\n\"\n",
        "            \"---\\n\"\n",
        "        )\n",
        "        md.append(section)\n",
        "    return \"\\n\".join(md)\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=forum_asistani,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=4, placeholder=\"Sorunuzu buraya yazƒ±n \\n(√∂r: \\\"ta≈üƒ±nmazda vergi borcu\\\" veya \\\"nikah\\\")\", label=\"Soru\"),\n",
        "    ],\n",
        "\n",
        "    outputs=gr.Markdown(label=\"Cevaplar\"),\n",
        "    title=\"Semantic Hybrid Search - Cevap Asistanƒ± - Tamer BAYAR 220205027\",\n",
        "    description=\"Daha √∂nce sorulmu≈ü sorulardan benzer olanlarƒ± bulup getirir.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "5bWQycso4cd4",
        "outputId": "81029605-7c73-42d7-e8ec-570fcc49132a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2080 adet soru y√ºklendi.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://de577d73e5ff4ff2e2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://de577d73e5ff4ff2e2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --diƒüer"
      ],
      "metadata": {
        "id": "ZsixwjHSeXvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#aray√ºzs√ºz test\n",
        "\n",
        "while True:\n",
        "    q = input(\"Soru (√ßƒ±kmak i√ßin q): \")\n",
        "    if q.lower() == \"q\":\n",
        "        break\n",
        "\n",
        "    results = sonuc_getir(q)\n",
        "\n",
        "    if results is None:\n",
        "        print(\"\\nBu soru i√ßin yeterli bilgi bulunmamaktadƒ±r.\\n\")\n",
        "        continue\n",
        "\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"\\nüîπ Sonu√ß {i} | Benzerlik: {r['score']:.3f}\")\n",
        "        print(\"Cevap:\")\n",
        "        print(r[\"cevap\"])\n"
      ],
      "metadata": {
        "id": "7MfWIMB21iH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standalone test - bir girdi ile bir sorgunun benzerliƒüini √ßabuk denemek i√ßin\n",
        "!pip install stopwords_tr\n",
        "\n",
        "import stopwords_tr as stp\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embed_model = SentenceTransformer(\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "def metin_temizle(text):\n",
        "    text = text.replace(\"ƒ∞\", \"i\").replace(\"I\", \"ƒ±\").lower()\n",
        "    for punc in [\".\", \",\", \"?\", \"!\", \":\", \";\", \"(\", \")\", \"\\\"\", \"'\", \"-\"]:\n",
        "        text = text.replace(punc, \" \")\n",
        "\n",
        "    kelimeler = text.split()\n",
        "    temiz_kelimeler = [k for k in kelimeler if k not in stp.stopwords() and len(k) > 2]\n",
        "    return temiz_kelimeler\n",
        "\n",
        "def benzerlik_hesapla(sorgu, konu):\n",
        "    vecs = embed_model.encode(\n",
        "        [sorgu, konu],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    base_score = float(np.dot(vecs[0], vecs[1]))\n",
        "\n",
        "    temiz_kelimeler = metin_temizle(sorgu)\n",
        "    n_temiz = len(temiz_kelimeler)\n",
        "\n",
        "    konu_clean = konu.replace(\"ƒ∞\", \"i\").replace(\"I\", \"ƒ±\").lower()\n",
        "\n",
        "    matches = sum(1 for k in temiz_kelimeler if k in konu_clean)\n",
        "\n",
        "    Bscore = base_score\n",
        "\n",
        "    if n_temiz > 0:\n",
        "        if matches > 0:\n",
        "            Bscore = Bscore + (matches / n_temiz * 0.30)\n",
        "        elif Bscore < 0.40:\n",
        "            Bscore = Bscore * 0.8\n",
        "\n",
        "    return {\n",
        "        \"sorgu\": sorgu,\n",
        "        \"konu\": konu,\n",
        "        \"base_score\": round(base_score, 4),\n",
        "        \"final_score\": round(min(Bscore, 1.0), 4),\n",
        "        \"matches\": matches,\n",
        "        \"temiz_kelimeler\": temiz_kelimeler\n",
        "    }\n",
        "\n",
        "\n",
        "konu  = \"The transfer of property ownership requires a formal deed at the land registry office.\"\n",
        "sorgu = \"Tapu dairesinde m√ºlkiyet devri nasƒ±l yapƒ±lƒ±r?\"\n",
        "\n",
        "sonuc = benzerlik_hesapla(sorgu, konu)\n",
        "\n",
        "print(f\"sorgu : {sonuc['sorgu']}\")\n",
        "print(f\"soru: : {sonuc['konu']}\")\n",
        "print(f\"temiz kelimelerelimeler : {sonuc['temiz_kelimeler']}\")\n",
        "print(f\"e≈üle≈üen kelimeler: {sonuc['matches']}\")\n",
        "print(f\"vekt√∂r skoru : {sonuc['base_score']}\")\n",
        "print(f\"final skor: {sonuc['final_score']}\")\n"
      ],
      "metadata": {
        "id": "RLCTKsSBZmNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#directory bulunamadƒ± hatasƒ± alƒ±rsanƒ±z √∂nce bu h√ºcreyi √ßalƒ±≈ütƒ±rƒ±n, olmazsa √ßalƒ±≈üma zamanƒ± baƒülantƒ±sƒ±nƒ± kesip tekrar baƒülamayƒ± deneyin.\n",
        "!rm -rf /root/.cache/huggingface/datasets\n",
        "!rm -rf /root/.cache/huggingface/hub"
      ],
      "metadata": {
        "id": "7CBwBQK6SE8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}